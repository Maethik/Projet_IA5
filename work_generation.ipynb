{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c956371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ctransformers huggingface-hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4696d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheo/.conda/envs/projet_ia5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du modèle: mistral-7b-instruct-v0.1.Q4_K_M.gguf...\n",
      "Modèle téléchargé dans: /home/matheo/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Définition du modèle à utiliser\n",
    "# On choisit un modèle Mistral 7B Instruct, quantifié en 4-bit (Q4_K_M)\n",
    "# C'est un bon équilibre performance / taille pour une utilisation CPU\n",
    "model_repo = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
    "model_file = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "# Télécharger le modèle (si pas déjà présent)\n",
    "print(f\"Téléchargement du modèle: {model_file}...\")\n",
    "model_path = hf_hub_download(repo_id=model_repo, filename=model_file)\n",
    "print(f\"Modèle téléchargé dans: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da8586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle sur le CPU (cela peut prendre un moment)...\n",
      "Modèle chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Configuration et chargement du modèle\n",
    "print(\"Chargement du modèle sur le CPU (cela peut prendre un moment)...\")\n",
    "\n",
    "# Configuration pour CTransformers\n",
    "# gpu_layers=0 signifie qu'aucune couche n'est déchargée sur le GPU (utilisation CPU pure)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    model_type=\"mistral\",\n",
    "    context_length=4096,  # Longueur de contexte max (texte + réponse)\n",
    "    gpu_layers=0          # <-- TRÈS IMPORTANT: Force l'utilisation du CPU\n",
    ")\n",
    "\n",
    "print(\"Modèle chargé avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b27196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire_style_epoque(texte_original: str, epoque_originale: str, epoque_cible: str) -> str:\n",
    "    \"\"\"\n",
    "    Utilise le LLM pour réécrire un texte dans le style d'une autre époque.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Un \"prompt\" système pour guider le modèle.\n",
    "    # Le format [INST]...[/INST] est spécifique à Mistral Instruct.\n",
    "    prompt = f\"\"\"[INST] Tu es un expert en histoire de la littérature française et un écrivain.\n",
    "Ta tâche est de réécrire le texte suivant, qui a été écrit dans le style de l'époque \"{epoque_originale}\", pour qu'il corresponde parfaitement au style (vocabulaire, grammaire, syntaxe) de l'époque \"{epoque_cible}\".\n",
    "Le sens, les idées et le contenu du texte original doivent être préservés au maximum. Ne fais que modifier le style.\n",
    "\n",
    "TEXTE ORIGINAL ({epoque_originale}):\n",
    "{texte_original}\n",
    "\n",
    "RÉÉCRITURE ({epoque_cible}):\n",
    "[/INST]\"\"\"\n",
    "\n",
    "    print(f\"Génération de la traduction stylistique de '{epoque_originale}' vers '{epoque_cible}'...\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # Appel du modèle\n",
    "    # 'temperature' contrôle la créativité (0.7 est un bon équilibre)\n",
    "    # 'max_new_tokens' est la longueur max de la réponse. Augmentez si vos textes sont longs.\n",
    "    # ATTENTION: La génération sur CPU sera LENTE.\n",
    "    reponse = llm(\n",
    "        prompt, \n",
    "        temperature=0.7, \n",
    "        max_new_tokens=1024, \n",
    "        top_k=40\n",
    "    )\n",
    "    \n",
    "    return reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3120a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération de la traduction stylistique de '2024' vers '1670 (style Molière ou La Fontaine)'...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- TEXTE ORIGINAL (2024) ---\n",
      "\n",
      "Salut les gars,\n",
      "J'ai checké le nouveau projet sur le drive, c'est vraiment un truc de ouf.\n",
      "Mon boss m'a dit que le client est super content et que le lancement est prévu pour ce week-end. \n",
      "On va pouvoir décompresser, c'est trop cool.\n",
      "\n",
      "\n",
      "--- TEXTE TRADUIT (1670 (style Molière ou La Fontaine)) ---\n",
      " Salut mes amis,\n",
      "J'ai vérifié le dernier projet sur mon bureau à la nuit, et je suis très content que cela soit bien arrivé.\n",
      "Mon patron m'a informé que le client est extrêmement satisfait et que le lancement est prévu pour ce weekend. \n",
      "On peut décomprimer, c'est trop magnifique.\n"
     ]
    }
   ],
   "source": [
    "# --- Exemple 1 : Moderne vers Ancien ---\n",
    "\n",
    "texte_moderne = \"\"\"\n",
    "Salut les gars,\n",
    "J'ai checké le nouveau projet sur le drive, c'est vraiment un truc de ouf.\n",
    "Mon boss m'a dit que le client est super content et que le lancement est prévu pour ce week-end. \n",
    "On va pouvoir décompresser, c'est trop cool.\n",
    "\"\"\"\n",
    "epoque_source_1 = \"2024\"\n",
    "epoque_cible_1 = \"1670 (style Molière ou La Fontaine)\"\n",
    "\n",
    "# Lancement de la génération\n",
    "texte_traduit_1 = traduire_style_epoque(texte_moderne, epoque_source_1, epoque_cible_1)\n",
    "\n",
    "# Affichage\n",
    "print(\"\\n--- TEXTE ORIGINAL (\" + epoque_source_1 + \") ---\")\n",
    "print(texte_moderne)\n",
    "print(\"\\n--- TEXTE TRADUIT (\" + epoque_cible_1 + \") ---\")\n",
    "print(texte_traduit_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a745126d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération de la traduction stylistique de '1720' vers '2024 (style email professionnel)'...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- TEXTE ORIGINAL (1720) ---\n",
      "\n",
      "Monseigneur,\n",
      "Je me fais l'honneur de vous adresser la présente missive afin de requérir votre bienveillance. \n",
      "Les affaires de votre domaine prospèrent, mais la météo capricieuse menace les récoltes.\n",
      "Je crains fort que la disette ne frappe nos manants si le ciel ne nous est point plus clément.\n",
      "Votre très humble et dévoué serviteur.\n",
      "\n",
      "\n",
      "--- TEXTE TRADUIT (2024 (style email professionnel)) ---\n",
      " Subject: Inquiry of Goodwill - Impact of Unfavorable Weather on Crop Yield\n",
      "\n",
      "Dear Sir,\n",
      "\n",
      "I trust this message finds you well and in good spirits. I am reaching out to kindly request your assistance with a matter of utmost concern for our community.\n",
      "\n",
      "As you are aware, the agricultural sector has been thriving under your leadership. However, recent weather conditions have posed an unprecedented challenge - excessive rainfall followed by sudden droughts have disrupted crop growth and yield. This unpredictable climate jeopardizes not only the livelihoods of our farmers but also food security for future generations.\n",
      "\n",
      "I urge you to consider taking proactive measures to mitigate the impact of these extreme weather events on agricultural productivity. It would be greatly appreciated if we could discuss potential solutions at your earliest convenience. \n",
      "\n",
      "Thank you in advance for considering this matter and for your continued commitment towards the welfare of our region.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "# --- Exemple 2 : Ancien vers Moderne ---\n",
    "\n",
    "texte_ancien = \"\"\"\n",
    "Monseigneur,\n",
    "Je me fais l'honneur de vous adresser la présente missive afin de requérir votre bienveillance. \n",
    "Les affaires de votre domaine prospèrent, mais la météo capricieuse menace les récoltes.\n",
    "Je crains fort que la disette ne frappe nos manants si le ciel ne nous est point plus clément.\n",
    "Votre très humble et dévoué serviteur.\n",
    "\"\"\"\n",
    "epoque_source_2 = \"1720\"\n",
    "epoque_cible_2 = \"2024 (style email professionnel)\"\n",
    "\n",
    "# Lancement de la génération\n",
    "texte_traduit_2 = traduire_style_epoque(texte_ancien, epoque_source_2, epoque_cible_2)\n",
    "\n",
    "# Affichage\n",
    "print(\"\\n--- TEXTE ORIGINAL (\" + epoque_source_2 + \") ---\")\n",
    "print(texte_ancien)\n",
    "print(\"\\n--- TEXTE TRADUIT (\" + epoque_cible_2 + \") ---\")\n",
    "print(texte_traduit_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_ia5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
